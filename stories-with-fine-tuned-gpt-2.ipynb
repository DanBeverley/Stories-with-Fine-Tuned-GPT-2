{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1415173,"sourceType":"datasetVersion","datasetId":792980}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nZero-Shot Transfer: The pre-training task for GPT-2 is solely language modeling. All the downstream language tasks are framed as predicting conditional probabilities and there is no task-specific fine-tuning.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport logging\nfrom tqdm import tqdm\nimport math\nimport argparse\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:29:50.244232Z","iopub.execute_input":"2024-09-10T02:29:50.245106Z","iopub.status.idle":"2024-09-10T02:29:50.250236Z","shell.execute_reply.started":"2024-09-10T02:29:50.245066Z","shell.execute_reply":"2024-09-10T02:29:50.249141Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/huggingface/transformers\n!pip install transformers/\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:29:50.251869Z","iopub.execute_input":"2024-09-10T02:29:50.252179Z","iopub.status.idle":"2024-09-10T02:30:31.359837Z","shell.execute_reply.started":"2024-09-10T02:29:50.252148Z","shell.execute_reply":"2024-09-10T02:30:31.358623Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"fatal: destination path 'transformers' already exists and is not an empty directory.\nProcessing ./transformers\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.0.dev0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9694583 sha256=ca40e01b7d793f5b76e31ef585354a19ddd9fc42f77e3866875ad71792206114\n  Stored in directory: /tmp/pip-ephem-wheel-cache-rhatsl6i/wheels/7e/b2/24/0b3be37b3b423a6f2fd25fd6368a1f4b0888942789c7e68bc6\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.0.dev0\n    Uninstalling transformers-4.45.0.dev0:\n      Successfully uninstalled transformers-4.45.0.dev0\nSuccessfully installed transformers-4.45.0.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\nparser.add_argument('--seed', type = int, default = 88888)\nparser.add_argument('--model_name', default = \"gpt-2\", type = str)\nparser.add_argument('--max_seq_length', default = 512 , type = int)\nparser.add_argument('--train_batch_size', default = 4, type = int)\nparser.add_argument('--valid_batch_size', default = 4, type = int)\nparser.add_argument('--num_train_epochs', default = 1, type = int)\nparser.add_argument('--warmup', default = .1 , type = float)\nparser.add_argument('--learning_rate', default = 5e-5, type = float)\nparser.add_argument('--input_text_path', default = '/kaggle/input/story-text', type = str)\nargs, _ = parser.parse_known_args()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:30:31.361360Z","iopub.execute_input":"2024-09-10T02:30:31.361706Z","iopub.status.idle":"2024-09-10T02:30:31.371079Z","shell.execute_reply.started":"2024-09-10T02:30:31.361671Z","shell.execute_reply":"2024-09-10T02:30:31.369993Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data\nCombine the prompt and story, do a little text clean. There are train, valid and test dataset in the original dataset. And the prompts and stories are in seperate files. For a example, the valid.wp_source has the writing prompts and valid.wp_target has the corresponding stories. The train dataset is very large. Since kaggle notebook limits the kernel running time to 3 hours. I decide to take the valid dataset as my train dataset, and the test dataset as valid dataset.\n\nIn order to feed the prompt an story together to GPT-2, I combine the prompts and stories togeter.Thus every line in the combined file includes the prompt and it's corresponding story.","metadata":{}},{"cell_type":"code","source":"DATAPATH = args.input_text_path\n\ndef combine_text(prompt, story):\n    fp = open(os.path.join(DATAPATH, prompt), encoding = 'utf8')\n    fs = open(os.path.join(DATAPATH, story), encoding = 'utf8')\n    prompts = fp.readlines()\n    stories = fs.readlines()\n    assert len(prompts) == len(stories), \"Unbalance length\"\n    combine = []\n    for i in range(len(prompts)):\n        combine.append(prompts[i].rstrip() + ' <sep> '+\" \".join(stories[i].split()[:300]))\n    return combine\n\n\n\ndef cleanpunctuation(s):\n    # Usage : Text cleaning with punctuations\n    for p in '!,.:;?':\n        s=s.replace(' '+p,p)\n    s=s.replace(' '+'n\\'t','n\\'t')\n    s=s.replace(' '+'\\'s','\\'s')\n    s=s.replace(' '+'\\'re','\\'re')\n    s=s.replace(' '+'\\'ve','\\'ve')\n    s=s.replace(' '+'\\'ll','\\'ll')\n    s=s.replace(' '+'\\'am','\\'am')\n    s=s.replace(' '+'\\'m','\\'m')\n    s=s.replace(' '+'\\' m','\\'m')\n    s=s.replace(' '+'\\'m','\\'m')\n    s=s.replace(' '+'\\' ve','\\'ve')\n    s=s.replace(' '+'\\' s','\\'s')\n    s=s.replace('<newline>','\\n')\n    return s\n\ntrain_text = combine_text('valid.wp_source', 'valid.wp_target')\ntrain_text = list(map(cleanpunctuation, train_text))\nvalid_text = combine_text(\"test.wp_source\", \"test.wp_target\")\nvalid_text = list(map(cleanpunctuation, valid_text))","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:30:31.374120Z","iopub.execute_input":"2024-09-10T02:30:31.374537Z","iopub.status.idle":"2024-09-10T02:30:35.068536Z","shell.execute_reply.started":"2024-09-10T02:30:31.374489Z","shell.execute_reply":"2024-09-10T02:30:35.067510Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize and load to dataloader¶\nGPT-2 uses BPE to tokenize the text squence.BPE merges frequently co-occurred byte pairs in a greedy manner. In order to let the sequences in the same batch have the same length, I set the max length of sequence as 512, and truncate the longer sequence and pad the shorter sequence. Since the tokenizer function only return the input_ids and attention_mask. For training purpose, I need to feed the labels(targets) to the model. So I create labels sequence for every input_ids squence. In the label sequence,I rule out the padding tokens by set it to -100 to avoid compute loss on them. And also GPT-2 will automatically shift the labels to the right to match the inputs_ids, so I don't need to deal with it.","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\ninputs_train = tokenizer(train_text, padding = True,\n                         truncation = True, max_length = args.max_seq_length)\ninputs_valid = tokenizer(valid_text, padding = True, truncation = True,\n                        max_length = args.max_seq_length)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:30:35.069828Z","iopub.execute_input":"2024-09-10T02:30:35.070146Z","iopub.status.idle":"2024-09-10T02:32:05.618735Z","shell.execute_reply.started":"2024-09-10T02:30:35.070112Z","shell.execute_reply":"2024-09-10T02:32:05.617927Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6fec7fcebc4987bc22ff221651eb3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2c0d9430394cbb8f08bde35d2f5d8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d17461b26b546709e079e8826b460dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a249ea526f464b99fb1e4ce73f92d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69dd68958f154640abc1f4c85bd442ff"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1614: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_labels(inputs):\n    labels = []\n    for ids, attention_mask in zip(inputs['input_ids'],inputs['attention_mask']):\n        label = ids.copy()\n        real_len = sum(attention_mask)\n        padding_len = len(attention_mask)-sum(attention_mask)\n        label[:] = label[:real_len]+[-100]*padding_len\n        labels.append(label)\n    inputs['labels'] = labels\n\ncreate_labels(inputs_train)\ncreate_labels(inputs_valid)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:32:05.620093Z","iopub.execute_input":"2024-09-10T02:32:05.620476Z","iopub.status.idle":"2024-09-10T02:32:06.486184Z","shell.execute_reply.started":"2024-09-10T02:32:05.620430Z","shell.execute_reply":"2024-09-10T02:32:06.485196Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class StoryDataset:\n    def __init__(self, inputs):\n        self.ids = inputs['input_ids']\n        self.attention_mask = inputs['attention_mask']\n        self.labels = inputs['labels']\n    def __len__(self):\n        return len(self.ids)\n    def __getitem__(self, item):\n        return [torch.tensor(self.ids[item], dtype = torch.long),\n               torch.tensor(self.attention_mask[item], dtype = torch.long),\n               torch.tensor(self.labels[item], dtype = torch.long)]","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:32:06.487412Z","iopub.execute_input":"2024-09-10T02:32:06.487750Z","iopub.status.idle":"2024-09-10T02:32:06.494765Z","shell.execute_reply.started":"2024-09-10T02:32:06.487716Z","shell.execute_reply":"2024-09-10T02:32:06.493753Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_batch_size = args.train_batch_size\nvalid_batch_size = args.valid_batch_size\ntrain_data = StoryDataset(inputs_train)\ntrain_dataloader = torch.utils.data.DataLoader(train_data, shuffle = False,\n                                              batch_size = train_batch_size)\nvalid_data = StoryDataset(inputs_valid)\nvalid_dataloader = torch.utils.data.DataLoader(valid_data, shuffle = True,\n                                              batch_size = valid_batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:32:06.496199Z","iopub.execute_input":"2024-09-10T02:32:06.496627Z","iopub.status.idle":"2024-09-10T02:32:06.506898Z","shell.execute_reply.started":"2024-09-10T02:32:06.496591Z","shell.execute_reply":"2024-09-10T02:32:06.505864Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.to(device)\nmodel.eval()\neval_loss = []\n\nfor inputs in tqdm(valid_dataloader, desc = \"eval\"):\n    d1, d2, d3 = inputs\n    d1 = d1.to('cuda')\n    d2 = d2.to('cuda')\n    d3 = d3.to('cuda')\n    \n    with torch.no_grad():\n        output = model(input_ids = d1, attention_mask = d2,\n                      labels = d3)\n        batch_loss = output[0]\n    eval_loss += [batch_loss.cpu().item()]\n    del batch_loss\neval_loss=np.mean(eval_loss)\nperplexity=math.exp(eval_loss)\nprint(f'The average perplexity for valid dataset before fine-tuning is {perplexity}') ","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:36:47.015265Z","iopub.execute_input":"2024-09-10T02:36:47.015919Z","iopub.status.idle":"2024-09-10T02:42:51.694067Z","shell.execute_reply.started":"2024-09-10T02:36:47.015880Z","shell.execute_reply":"2024-09-10T02:42:51.693105Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac6f2dd4bc44d6982a726709edd3990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a421ed28386e428db6f89b6912b64d87"}},"metadata":{}},{"name":"stderr","text":"eval: 100%|██████████| 3785/3785 [06:01<00:00, 10.47it/s]","output_type":"stream"},{"name":"stdout","text":"The average perplexity for valid dataset before fine-tuning is 39.27932676546657\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's pick a prompt from the valid dataset and input it into the model, have the model generate a 300 words long story. The output stories is really great! I use the generate method comes with the model. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling. The meanings of key arguments are below:\n* 1)do_sample: if set to False greedy decoding is used.\n* 2)The temperature is used to module the next token probabilities.\n* 3)top_k is the number of highest probability vocabulary tokens to keep for top-k-filtering.\n* 4)top_p is the cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling.\n* 5)repetition_penalty is the parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty.","metadata":{}},{"cell_type":"code","source":"prompt = valid_text[300][:valid_text[300].find('<sep>')]\ntarget = valid_text[300][valid_text[300].find(\"<sep>\")+5:]\n\ndef generate_story(prompt, target, k = 0, p = .9, output_length = 300, temperature = 1,\n                  num_return_sequences = 3, repitition_penalty = 1.):\n    print(\"| Prompt |\\n\")\n    print(prompt + \"\\n\")\n    print(\"| Generated |\\n\")\n    print(target + \"\\n\")\n    encoded_prompt = tokenizer.encode(prompt, add_special_tokens = False,\n                                     return_tensors ='pt')\n    model.to('cpu')\n    model.eval()\n    output_sequences = model.generate(\n    input_ids = encoded_prompt,\n    max_length = output_length,\n    temperature = temperature,\n    top_k = k,\n    top_p = p,\n    repetition_penalty = repitition_penalty,\n    do_sample = True,\n    num_return_sequences = num_return_sequences)\n    \n    if len(output_sequences.shape)>2:\n        output_sequences.squeeze_()\n    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n        print(\"|| Generated Sequence {} ||\".format(generated_sequence_idx+1))\n        generated_sequece = generated_sequence.tolist()\n        # Decode text\n        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces = True)\n        # Remove all text after eos token\n        text = text[:text.find(tokenizer.eos_token)]\n        print(text)\n\ngenerate_story(prompt, target)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:59:15.225387Z","iopub.execute_input":"2024-09-10T02:59:15.226133Z","iopub.status.idle":"2024-09-10T02:59:36.603683Z","shell.execute_reply.started":"2024-09-10T02:59:15.226092Z","shell.execute_reply":"2024-09-10T02:59:36.602439Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"| Prompt |\n\nChildren's logic dictates the way the world works. [ WP ] \n\n| Generated |\n\n “ That ’ s not an option I ’ m currently willing to exercise. ” \n \n I pinch the bridge of my nose to stave off the headache building behind my eyes. If this goes on much longer, I ’ m gon na have to start to start cutting back on the vegetables. \n \n “ She ’ s dangerous, Jimmy. You know that. You ’ ve seen it. Dealt with it first hand. She just doesn ’ t play by anyone ’ s rules. ” \n \n Ali finished off her sucker and unwrapped a fresh one, offering it to me. I declined. I ’ d sworn off the things after my third cavity scare. That one saw me at the dentist for the third time in as many months. I don ’ t care what my dad says, I know that guy is evil. Who owns a drill like that? A murderer, that ’ s who. I still hear the damn thing in my nightmares. \n \n While she savored the smooth flavor of blue-raspberry, I pondered her words. We both knew she was right. The situation was spiralling out of control. The details of our latest reports flooded my mind, even as I sing songed a few La-La ’ s to block them out. \n \n Our perp is a wildcard and damn near untraceable. Hide and seek skills like none I ’ ve ever seen. In another life she might have made the perfect detective, but as it was, no one could trust her as far as they could throw her, and she always went\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"|| Generated Sequence 1 ||\nChildren's logic dictates the way the world works. [ WP ]  But does this work for almost anybody?\nHere's what used to be this passage on why Religion is the moral order, again out of all the arguments concerning why:\n\"Deeper stages of human society do not yield either less liberty or more safety, as does a breach of the rules and norms of an open society, or more there are simply more bugs and ornaments left to reduce the number of ambiguities in particular persons, the degrees of alarm in the mind, and the likelihood of unintended or mischievous incidents. By contriving such practicality to enable mankind to maintain its basic functioning of safety it is a more effective means of controlling external threats and maintaining the dignity of conscience. That truth about purity and chastity is now shattered. This is the first stage of religious morality which is at the root of the highest of all other life. So it is the first stage of religious morality, in virtue of which [ there are] no tyrannical moral laws which would challenge the principle of self-improvement\"\nThere is more than just power in humanity, there's nothing in the doctrine of homosexuality. Love is not connected to natural sexual desires, very few couples can fathom homosexuality is related to birth control. Because women are forced into marriage they tend to love their husbands more, the more likely it is that a child will develop feelings of love and more love will develop into marriage\n|| Generated Sequence 2 ||\nChildren's logic dictates the way the world works. [ WP ] __________________\n\nIf it were really so, then whatever Universal Music License was used to provide access to it would have already been in force only fifteen years ago, and that would have been without hardening the way of things. Wouldn't it be preferable to simply give Universal Music Control, either to the musicians, to filter, \"not just access, but also possession\" to producers. Who would want that? Most members of the music public are the same types of people: musicians. Universal Music Control could sound like a much simpler system, but it isn't.\n\nI'd say this: If you're even halfway through your college lecture notes, you probably don't understand what it is. It's not what happened to the lives of James Randi and Isaac Asimov; it's just the fact that when they tried to take control of the record business and build a head start around it, they found that there was no direct connection between the results they were looking for and what their customers thought about them.\n\nAnd this is what modern, futuristic music allows you to do. And the second degree is that you just leave the records at your desk and plan out exactly where you want to go, making it a critical part of what you listen to.\n\nIndeed, of course, it's convenient for most people to ignore it. Even before VHS, two decades ago, the video record maker wa\n|| Generated Sequence 3 ||\nChildren's logic dictates the way the world works. [ WP ]  \"War is people's control\" [ DS ]\nOn the Isle of Man one of the Water Watchmen have nightmares about drowning at sea. They think they can do it, but they cannot understand why (Alone with a frustrated spirit) they are drowning. They would love nothing more than to see the other's despair, and that's their argument for chasing them to safety. Two [DB] walk along an Isle of Man lake and hear the whole thing come down. The body is different than the blood of each and every one, but that's not it. Perhaps you're a part of the Horde. The team hears a ring of rustling in the rain (officially yes), eventually they come across the island of Arere, where they've been with a couple of the River Warriors from Uther, who were trying to use the world's good so as to control the undead. All the land is puddles of cobwebs, broken into part by the Horde and by other nameless tribe. Some are under a guardian spirit with the ability to make chandeliers, others, apparently by level, glow with red light.\nBelfast Rising brings you Bloodborne II: A Stormwind Map set. Click.. You're the world's top assassin. Do the quest like any of us can: start by killing the Goblins. Continue the quests in a different world; mos\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tune the model\nThe number of training samples is 15620. With one GPU to train the model, it tooks about 21 minutes to run 1 epoch. After 1 epoche learning, the perplexity for valid dataset is about 24, which is better than the score before fine- tuning.","metadata":{}},{"cell_type":"code","source":"num_train_epochs = args.num_train_epochs\ntraining_steps_per_epoch = len(train_dataloader)\ntotal_num_training_steps = int(training_steps_per_epoch*num_train_epochs)\nweight_decay = 0\nlearning_rate = args.learning_rate\nadam_epsilon = 1e-8\nwarmup_steps = int(total_num_training_steps*args.warmup)\nno_decay = ['bias', \"LayerNorm.weight\"]\n\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        \"weight_decay\": weight_decay,\n    },\n    {\n        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:59:37.128531Z","iopub.execute_input":"2024-09-10T02:59:37.129036Z","iopub.status.idle":"2024-09-10T02:59:37.140736Z","shell.execute_reply.started":"2024-09-10T02:59:37.129001Z","shell.execute_reply":"2024-09-10T02:59:37.139901Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print(\"***** Running training *****\")\nprint(\"  Total_num_training_step = {}\".format(total_num_training_steps))\nprint(\"  Num Epochs = {}\".format(num_train_epochs))\nprint(f\"  Train_batch_size per device = {train_batch_size}\")\nprint(f\"  Valid_batch_size per device = {valid_batch_size}\")\nmodel.to('cuda')\nfor epoch in range(num_train_epochs):\n    print(f\"Start epoch{epoch+1} of {num_train_epochs}\")\n    train_loss=0\n    epoch_iterator = tqdm(train_dataloader,desc='Iteration')\n    model.train()\n    model.zero_grad()    \n    for _, inputs in enumerate(epoch_iterator):        \n        d1,d2,d3=inputs\n        d1=d1.to('cuda')\n        d2=d2.to('cuda')\n        d3=d3.to('cuda')\n        output = model(input_ids=d1, attention_mask=d2,labels=d3)\n        batch_loss=output[0]\n        batch_loss.backward()\n        optimizer.step()\n        scheduler.step()\n        model.zero_grad()\n        train_loss+=batch_loss.item()\n        epoch_iterator.set_description('(batch loss=%g)' % batch_loss.item())\n        del batch_loss\n    print(f'Average train loss per example={train_loss/training_steps_per_epoch} in epoch{epoch+1}')    \n    print(f'Starting evaluate after epoch {epoch+1}')\n    eval_loss=[]    \n    model.eval()    \n    for inputs in tqdm(valid_dataloader, desc=\"eval\"):\n        d1,d2,d3=inputs\n        d1=d1.to('cuda')        \n        d2=d2.to('cuda')\n        d3=d3.to('cuda')\n        with torch.no_grad():\n            output = model(input_ids=d1, attention_mask=d2,labels=d3)\n            batch_loss=output[0]\n        eval_loss+=[batch_loss.cpu().item()]\n        del batch_loss\n    eval_loss=np.mean(eval_loss)\n    perplexity=math.exp(eval_loss)\n    print(f'Average valid loss per example={eval_loss} in epoch{epoch+1}')    \n    print(f'Perplextiy for valid dataset in epoch{epoch+1} is {perplexity}')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T02:59:37.142904Z","iopub.execute_input":"2024-09-10T02:59:37.143303Z","iopub.status.idle":"2024-09-10T03:27:08.097231Z","shell.execute_reply.started":"2024-09-10T02:59:37.143259Z","shell.execute_reply":"2024-09-10T03:27:08.096263Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"***** Running training *****\n  Total_num_training_step = 3905\n  Num Epochs = 1\n  Train_batch_size per device = 4\n  Valid_batch_size per device = 4\nStart epoch1 of 1\n","output_type":"stream"},{"name":"stderr","text":"(batch loss=2.89359): 100%|██████████| 3905/3905 [21:30<00:00,  3.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average train loss per example=3.2843164310870496 in epoch1\nStarting evaluate after epoch 1\n","output_type":"stream"},{"name":"stderr","text":"eval: 100%|██████████| 3785/3785 [06:00<00:00, 10.49it/s]","output_type":"stream"},{"name":"stdout","text":"Average valid loss per example=3.182704730140958 in epoch1\nPerplextiy for valid dataset in epoch1 is 24.11188156833926\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generate Stories\nUse the fine tuned model to generate stories with the same prompt before fine-tune","metadata":{}},{"cell_type":"code","source":"prompt = valid_text[300][:valid_text[300].find('<sep>')]\ntarget = valid_text[300][valid_text[300].find('<sep>')+5:]\ngenerate_story(prompt, target)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T03:40:16.890229Z","iopub.execute_input":"2024-09-10T03:40:16.890629Z","iopub.status.idle":"2024-09-10T03:40:38.156931Z","shell.execute_reply.started":"2024-09-10T03:40:16.890592Z","shell.execute_reply":"2024-09-10T03:40:38.156118Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"| Prompt |\n\nChildren's logic dictates the way the world works. [ WP ] \n\n| Generated |\n\n “ That ’ s not an option I ’ m currently willing to exercise. ” \n \n I pinch the bridge of my nose to stave off the headache building behind my eyes. If this goes on much longer, I ’ m gon na have to start to start cutting back on the vegetables. \n \n “ She ’ s dangerous, Jimmy. You know that. You ’ ve seen it. Dealt with it first hand. She just doesn ’ t play by anyone ’ s rules. ” \n \n Ali finished off her sucker and unwrapped a fresh one, offering it to me. I declined. I ’ d sworn off the things after my third cavity scare. That one saw me at the dentist for the third time in as many months. I don ’ t care what my dad says, I know that guy is evil. Who owns a drill like that? A murderer, that ’ s who. I still hear the damn thing in my nightmares. \n \n While she savored the smooth flavor of blue-raspberry, I pondered her words. We both knew she was right. The situation was spiralling out of control. The details of our latest reports flooded my mind, even as I sing songed a few La-La ’ s to block them out. \n \n Our perp is a wildcard and damn near untraceable. Hide and seek skills like none I ’ ve ever seen. In another life she might have made the perfect detective, but as it was, no one could trust her as far as they could throw her, and she always went\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"|| Generated Sequence 1 ||\nChildren's logic dictates the way the world works. [ WP ] \n \n In a perfect world we all feel superior to the man or woman we love. We are kings and queens, blessed with a limitless amount of power and thousands upon thousands of genes. We are blessed with vast, red-colored planets with the ability to sustain life billions of miles away. \n \n In a perfect world we all would be born who have the same basic human genetics but who neither used as much power nor knew of the dangers of genetic modification. But that is not the case. The balance between good and evil are always tipped. \n \n In a perfect world, every generation has the same ultimate capability - we are kings and queens. But the balance is tipped wrong. The planet is barren with only nature that we can manipulate - and when that happens, we lose control over our chosen masters. Then, we lose the world, and life continues. \n \n The truth is, no one ever truly believes in us. We have been through so much, we don't care. We make choices. We control our lives, and this is our job. As a group, we understand and care. \n \n We are the true rulers of this world. Not because we are necessarily good, but because we are able to manipulate our environments and words. \n \n Every single person in this world, regardless of his or her race, sex or language, wil\n|| Generated Sequence 2 ||\nChildren's logic dictates the way the world works. [ WP ] \n \n Humanity must commit to the deadliest weapon, each person born will die in order to achieve the greatest number of weapon. Every person in the world must submit to a test which will see what their total score will be, and if they do so they are marked for death. If a person does not show up the test is to be used. `` Show me another human? '' is how the test is administered. \n \n Two humans are sent to test if they can pass the test, one of them is a drug user and the other is a student at a prestigious university. One of the researchers is making a joke about a drug that kills people. The student comments that it kills him, but the student's name is Damore and the one that replies he is a drug user. Damore passes the test but the student is not certain if he has the ability to show that he can kill people. Damore decides that he is `` pretty bad '' and the student reacts by telling him to `` wait a bit. '' Damore changes the subject in order to show that Damore is a drug user. He uses a mannequin and a car to show that he has the ability to take lives. Damore passes the test but the student is not certain if he has the ability to take lives. Damore accepts the message from the professor and takes the car. He cuts the windshield of th\n|| Generated Sequence 3 ||\nChildren's logic dictates the way the world works. [ WP ] \n _What's happening to us now: the Arcanum and Subterranean Spire are critically damaged. They would not be fighting the invasion alone. ^^_> `` Survival is fine now '' Pyogenyi said as she placed her knife on her friend. `` The plants will not spread. *They* do not grow back. '' She knew that. Every part of the world they watered would continue their growing. All they left was her new one that always responded well to them. \n \n They would not take the same actions without her cooperation. From the confines of their little cafe, there were one in every bar and at every table. Their problems were small, but they were watching what happened. The Arcanum exploded like a booster pack. They had spent thousands of years. Many had stolen it from this world, it was eventually forgotten, nothing was left. It was once free for them to set up bases, trade with the other planets that had enslaved it. Humans were free to choose to. People would move freely. \n \n Fleeing to the beach, which was a nice way of escaping the war. The Earth's hydrocarbons were gone. No sky, no plants. The sky was cracked in the ocean. Nothing at all was left to keep the eco, but the planet had maintained life, it didn't need every other life, so everything wa\n","output_type":"stream"}]}]}